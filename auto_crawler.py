import requests
import zipfile
import os
import random
import time
from utils.load_stations import load_stations
from alive_progress import alive_bar
from logs.logger import setup_logger
from datetime import datetime


# è¨­å®š logger
logger = setup_logger()

class Crawler:
    def __init__(self, station_id, year, station_name):
        self.station_fullname = f"{year}_{station_id}_{station_name}_tide"

        # æŒ‡å®šä¿å­˜å£“ç¸®æª”æ¡ˆçš„è·¯å¾‘
        self.zip_file_path = "save/zip/saved_file.zip"

        # æŒ‡å®šè§£å£“ç¸®å¾Œçš„ç›®æ¨™è³‡æ–™å¤¾
        self.extract_to_path = "save/csv/"

        # çˆ¬èŸ²é€£çµ
        self.url = "https://ocean.cwa.gov.tw/V2/data_interface/download/download_file"

        # çˆ¬èŸ²å¸¶å…¥åƒæ•¸
        self.params = {
            "dataset": "Tide-his", # è³‡æ–™é›†åç¨±
            "factor": "t",
            "download_type": "csv", # ä¸‹è¼‰æª”æ¡ˆçš„æ ¼å¼
            "begin": year, # èµ·å§‹å¹´ä»½
            "end": year, # çµæŸå¹´ä»½
            "station_id": station_id, # ç«™è™Ÿ
        }

        
    # å°‡å£“ç¸®æª”ä¸‹è¼‰ä¸‹ä¾†
    def download(self):
        # é€érequest + streamå°‡æª”æ¡ˆä¸‹è¼‰ä¸‹ä¾†
        response = requests.get(self.url, params=self.params, stream=True)

        # æª¢æŸ¥è«‹æ±‚æ˜¯å¦æˆåŠŸ
        if response.status_code == 200:

            # é–‹å•Ÿæª”æ¡ˆï¼Œä¸¦å¯«å…¥å¾è«‹æ±‚ä¸­å–å¾—çš„å…§å®¹
            with open(self.zip_file_path, "wb") as file:
                for chunk in response.iter_content(chunk_size=8192):
                    file.write(chunk)

            logger.info(f"ğŸ“¥ [{self.station_fullname}]å£“ç¸®æª”ä¸‹è¼‰æˆåŠŸï¼Œæª”æ¡ˆå·²ä¸‹è¼‰åˆ°: {self.zip_file_path}")

        else:
            logger.error(f"â›” ä¸‹è¼‰å¤±æ•—ï¼Œè«‹ç¢ºèªæ˜¯å¦æœ‰è©²ç«™é»ID: {self.station_fullname}")
    
    # è§£å£“ç¸®
    def extract(self):
        # æª¢æŸ¥ç›®æ¨™è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨å‰‡å‰µå»º
        if not os.path.exists(self.extract_to_path):
            os.makedirs(self.extract_to_path)

        try:
            # ä½¿ç”¨ zipfile è§£å£“ç¸®
            with zipfile.ZipFile(self.zip_file_path, 'r') as zip_ref:
                zip_ref.extractall(self.extract_to_path)
            
            logger.info(f"âœ… [{self.station_fullname}]å£“ç¸®æˆåŠŸ (è·¯å¾‘: {self.extract_to_path})")
        except Exception as e:
            logger.error(f"â›” è§£å£“ç¸®å¤±æ•—: {e}")


if __name__ == "__main__":
    # ç«™é»åˆ—è¡¨
    stations_hash_map = load_stations()

    # å–å¾—ä»Šå¹´å¹´ä»½
    year = datetime.now().year

    # éš¨æ©Ÿç­‰å¾…æ™‚é–“
    sleep_time = random.randint(2, 8)
    
    with alive_bar(len(stations_hash_map)) as bar:
        for station_id, value in stations_hash_map.items():
            # ç«™é»åç¨±
            station_name = value.get("name", None)

            # å‰µå»ºçˆ¬èŸ²ç‰©ä»¶
            crawler = Crawler(station_id, year, station_name)

            # ä¸‹è¼‰å£“ç¸®æª”æ¡ˆ
            crawler.download()

            # å°‡å£“ç¸®æª”è§£å£“æˆcsvæª”
            crawler.extract()


            # ç­‰å¾…éš¨æ©Ÿæ™‚é–“
            time.sleep(sleep_time)

            bar()